# EOFuser project with OmniSat

[![python](https://img.shields.io/badge/-Python_3.10+-blue?logo=python&logoColor=white)](https://github.com/pre-commit/pre-commit)
[![pytorch](https://img.shields.io/badge/PyTorch_2.0+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/)
[![lightning](https://img.shields.io/badge/-Lightning_2.2+-792ee5?logo=pytorchlightning&logoColor=white)](https://pytorchlightning.ai/)
[![hydra](https://img.shields.io/badge/Config-Hydra_1.3-89b8cd)](https://hydra.cc/)
[![license](https://img.shields.io/badge/License-MIT-green.svg?labelColor=gray)](https://github.com/ashleve/lightning-hydra-template#license)

EOFuser project for drone and satellite fusion, based on [OmniSat](https://github.com/gastruc/OmniSat)

## Description

<p align="center">
  <img src="https://github.com/gastruc/OmniSat/assets/1902679/9fc20951-1cac-4891-b67f-53ed5e0675ad" width="500" height="250">
</p>

## Project Structure

The directory structure of new project looks like this:

```
â”œâ”€â”€ configs                   <- Hydra configs
â”‚   â”œâ”€â”€ callbacks                <- Callbacks configs
â”‚   â”œâ”€â”€ dataset                  <- Data configs
â”‚   â”œâ”€â”€ debug                    <- Debugging configs
â”‚   â”œâ”€â”€ exp                      <- Experiment configs
â”‚   â”œâ”€â”€ extras                   <- Extra utilities configs
â”‚   â”œâ”€â”€ hparams_search           <- Hyperparameter search configs
â”‚   â”œâ”€â”€ hydra                    <- Hydra configs
â”‚   â”œâ”€â”€ local                    <- Local configs
â”‚   â”œâ”€â”€ logger                   <- Logger configs
â”‚   â”œâ”€â”€ model                    <- Model configs
â”‚   â”œâ”€â”€ paths                    <- Project paths configs
â”‚   â”œâ”€â”€ trainer                  <- Trainer configs
â”‚   â”‚
â”‚   â”œâ”€â”€ config.yaml            <- Main config for training
â”‚   â””â”€â”€ eval.yaml              <- Main config for evaluation
â”‚
â”œâ”€â”€ data                   <- Project data
â”‚
â”œâ”€â”€ logs                   <- Logs generated by hydra and lightning loggers
â”‚
â”œâ”€â”€ src                    <- Source code
â”‚   â”œâ”€â”€ data                     <- Data scripts
â”‚   â”œâ”€â”€ models                   <- Model scripts
â”‚   â”œâ”€â”€ utils                    <- Utility scripts
â”‚   â”‚
â”‚   â”œâ”€â”€ eval.py                  <- Run evaluation
â”‚   â”œâ”€â”€ train_pastis_20.py       <- Run training on 20% pastis dataset
â”‚   â””â”€â”€ train.py                 <- Run training
â”‚
â”œâ”€â”€ .env.example              <- Example of file for storing private environment variables
â”œâ”€â”€ .gitignore                <- List of files ignored by git
â”œâ”€â”€ .project-root             <- File for inferring the position of project root directory
â”œâ”€â”€ environment.yaml          <- File for installing conda environment
â”œâ”€â”€ Makefile                  <- Makefile with commands like `make train` or `make test`
â”œâ”€â”€ pyproject.toml            <- Configuration options for testing and linting
â”œâ”€â”€ requirements.txt          <- File for installing python dependencies
â”œâ”€â”€ setup.py                  <- File for installing project as a package
â””â”€â”€ README.md
```

## ðŸš€Quickstart

```bash
# clone project

# [OPTIONAL] create conda environment
conda create -n eofuser_omni python=3.10
conda activate eofuser_omni

# install requirements
pip install -r requirements.txt

# Create data folder where you can put your datasets
mkdir data
# Create logs folder
mkdir logs
```

## Usage

Every experience of the paper has its own config. Feel free to explore configs/exp folder


Pass your data path to files with paths.data_path=<path>
```bash
python src/train.py exp=TSAITS_OmniSAT #to run OmniSAT pretraining on TreeSatAI-TS
#trainer.devices=X to change the number of GPU you want to train on
#trainer.num_workers=16 to change the num_workers available
#dataset.global_batch_size=16 to change global batch size (ie batch size that will be distributed across all GPUS)
#offline=True to run in offline mode from wandb
#max_epochs=1 to change the number maximum of epochs

python src/train.py exp=TSAITS_OmniSAT #to run OmniSAT finetuning on TreeSatAI-TS
#model.name=OmniSAT_MM  to change model name for logging
#partition=1.0 to change the percentage on training data you want to use

# All these parameters and more can be changed from the config file
```

To run 20% experiments on PASTIS-HD, you have to run

```bash
python src/train_pastis_20.py exp=Pastis_ResNet #to run a ResNet on PASTIS-HD
#partition parameter does not change anything on PASTIS-HD
```
## Citation

To refer to our work, please cite:

```text
TBA
```

To refer to original OmniSat work, please cite
```
@article{astruc2024omnisat,
  title={Omni{S}at: {S}elf-Supervised Modality Fusion for {E}arth Observation},
  author={Astruc, Guillaume and Gonthier, Nicolas and Mallet, Clement and Landrieu, Loic},
  journal={ECCV},
  year={2024}
}
```


## Acknowledgements
- This project was built on foundation of [OmniSat](https://github.com/gastruc/OmniSat).
- This project was built using [Lightning-Hydra template](https://github.com/ashleve/lightning-hydra-template).
- The original code of [TreeSat](https://git.tu-berlin.de/rsim/treesat_benchmark)
- The transformer structures come from [timm](https://github.com/huggingface/pytorch-image-models)
- The implementation of UT&T come from the [FLAIR 2 challenge repository](https://github.com/IGNF/FLAIR-2)
- The implementation of SatMAE and ScaleMAE come from [USat](https://github.com/stanfordmlgroup/USat)
- The Croma implementation comes from [Croma](https://github.com/antofuller/CROMA)
- The Relative Positional Encoding implementation come from [iRPE](https://github.com/microsoft/Cream/tree/main/iRPE)
- The Pse, ltae, UTAE come from [utae-paps](https://github.com/VSainteuf/utae-paps/tree/main)

<br>
